# CV_Multi_Relation_Classification

### Main Task

<img src="./pics/Screen Shot 2021-10-11 at 1.06.36 PM.png" alt="Screen Shot 2021-10-11 at 1.06.36 PM" width="200" />

- **INPUT** : Subject bounding box ì™€ Object bounding box ì •ë³´ê°€ í¬í•¨ëœ í•œ ì¥ì˜ ì´ë¯¸ì§€
- **OUTPUT** : Predicate class 70ê°œì— ëŒ€í•œ ì˜ˆì¸¡ vector ( ê° value ë²”ìœ„ : [0, 1] )



### Dataset

- **VRD (Visual Relationship Detection dataset)**

  - Download : https://drive.google.com/file/d/1vPncv8A5m1P_iMFrhdzeANZSSgLtcpSc/view?usp=sharing 

  - *Images* :

    - Train (train_images) : 2999ê°œ

      ( `002424.jpg` ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ ) 

    - Validation (val_images) : 1000ê°œ 

    - Test (test_images) : 1000ê°œ 

  - *Relation Annotation* :

    <img src="./pics/Screen Shot 2021-10-11 at 1.04.47 PM.png" alt="Screen Shot 2021-10-11 at 1.04.47 PM" width="300" />

    ```python
    {...
    FILENAME: [...
    	{'subject': {'category': CATEGORY_ID, 'bbox': [XMIN, YMIN, XMAX, YMAX]},
    	 'predicate': [PREDICATE_ID, ...],
    	 'object': {'category': CATEGORY_ID, 'bbox': [XMIN, YMIN, XMAX, YMAX]},
    	}
    	...]
    ...}
    ```

    - Object Class ID : 0~99
    - Predicate Class ID : 0~69
    - Training Data : 20225ê°œ (ì¦‰, 1ê°œ train_image ë‹¹ í‰ê·  20225/2999 â‰ˆ 7ê°œ Relation pairê°€ ìˆìŒ)



### Models

#### CNNì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ êµ¬ì¡° ( `model_ver: 1` )

- `predicates.json` íŒŒì¼ ì•ˆì— ìˆëŠ” ì—¬ëŸ¬ predicate classë“¤ì„ ë³´ì•˜ì„ ë•Œ, ë°©í–¥ê³¼ ê´€ë ¨ìˆëŠ” predicateë“¤ì´ ëŒ€ë‹¤ìˆ˜ì˜€ë‹¤ (next toï¼Œunderï¼Œon the top ofï¼Œon the right of ë“±ë“±). ê·¸ë˜ì„œ Subject bounding boxì—ì„œ Object bounding box ê¹Œì§€ì˜ ë°©í–¥ì´ ì¤‘ìš”í•œ featureì¤‘ í•˜ë‚˜ë¼ ìƒê°í–ˆë‹¤.
- ë°©í–¥ê³¼ ë”ë¶ˆì–´ Subjectì™€ Objectì˜ í¬ê¸°ì™€ ê´€ë ¨ìˆëŠ” predicateë“¤ë„ ë§ì´ ì°¾ì•„ë³¼ ìˆ˜ ìˆì—ˆë‹¤(taller than, skate on, hold, wear ë“±ë“±). ëª¨ë“  ì´ë¯¸ì§€ë“¤ì˜ scaleì´ ëª¨ë‘ ë‹¤ë¥¸ì ì„ ê³ ë ¤í•´ì„œ scaleì— ë…ë¦½ì ì¸  Subjectì™€ Objectì˜ ë¹„ìœ¨ê³¼ ë©´ì  ì •ë³´ë„ ì¤‘ìš”í•œ featureë¼ê³  ìƒê°í–ˆë‹¤.

##### [ë°ì´í„° ì „ì²˜ë¦¬]

<img src="./pics/Screen Shot 2021-10-09 at 6.33.11 PM.png" alt="Screen Shot 2021-10-09 at 6.33.11 PM" style="zoom: 40%;" />

- ***Histogram feature*** : 

  í•œ ì´ë¯¸ì§€ì—ì„œ ê° Subjectì™€ Objectì˜ Bounding box ë‚´ ì˜ì—­ì˜ RGB Color Histogramê³¼ Gradient Histogramë¥¼ ê³„ì‚°í•œë‹¤. (`cv2.calcHist()` , `skimage.hog()` ì‚¬ìš©)

- ***Direction feature*** : 

  <img width="204" alt="example" src="https://user-images.githubusercontent.com/44460142/85228569-0f2fd400-b41f-11ea-93ce-f97f034d3e9e.png">

  One-hot encode í˜•ì‹ì˜ feature arrayì´ë©° arrayì˜ ê° indexëŠ” ë‚˜ì¹¨ë°˜ì˜ 16ê°œ ë°©í–¥ (N, NNE, NE, ENE, E ë“±...)ì„ ì˜ë¯¸í•œë‹¤. í•œ ì´ë¯¸ì§€ì—ì„œ Subject bounding boxì˜ ì¤‘ì‹¬ì„ ê¸°ì¤€ìœ¼ë¡œ Object bounding box ì¤‘ì‹¬ê¹Œì§€ì˜ ë°©í–¥ì„ ê³„ì‚°í•˜ì—¬ ëŒ€ì‘í•˜ëŠ” indexì˜ ê°’ì´ 1ì´ ëœë‹¤.

- ***Bounding box Ratio & Area feature*** :

  ë³¸ Feature arrayì˜ ì•ì— 7ê°œ indexëŠ” One-hot encode í˜•ì‹ì´ë©° Subjectì™€ objectì˜ bounding box ê°€ë¡œ,ì„¸ë¡œ ê¸¸ì´ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì•ì— 7ê°œ indexê°’ì´ 1ì´ë˜ëŠ” ê¸°ì¤€ì€ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡± í•  ê²½ìš° ì„¤ì •ëœë‹¤ :

  Index[0] :  `Subject bboxì˜ xì¶• ê¸¸ì´ > Subject bboxì˜ yì¶• ê¸¸ì´*2` 

  Index[1] :  `Subject bboxì˜ yì¶• ê¸¸ì´*2 > Subject bboxì˜ xì¶• ê¸¸ì´ > Subject bboxì˜ yì¶• ê¸¸ì´  ` 

  Index[2] :  `Subject bboxì˜ xì¶• ê¸¸ì´*2 > Subject bboxì˜ yì¶• ê¸¸ì´ > Subject bboxì˜ xì¶• ê¸¸ì´  ` 

  Index[3] :  `Subject bboxì˜ yì¶• ê¸¸ì´ > Subject bboxì˜ xì¶• ê¸¸ì´*2` 

  Index[4] :  `Object bboxì˜ xì¶• ê¸¸ì´ > Object bboxì˜ yì¶• ê¸¸ì´*2` 

  Index[5] :  `Object bboxì˜ yì¶• ê¸¸ì´*2 > Object bboxì˜ xì¶• ê¸¸ì´ > Object bboxì˜ yì¶• ê¸¸ì´  ` 

  Index[6] :  `Object bboxì˜ xì¶• ê¸¸ì´*2 > Object bboxì˜ yì¶• ê¸¸ì´ > Object bboxì˜ xì¶• ê¸¸ì´  ` 

  Index[7] :  `Object bboxì˜ yì¶• ê¸¸ì´ > Object bboxì˜ xì¶• ê¸¸ì´*2` 

  Feature arrayì˜ ë§ˆì§€ë§‰ indexëŠ” `Subject bboxì˜ ë©´ì  / Object bboxì˜ ë©´ì `  ê°’ì´ë‹¤.

  

##### [ëª¨ë¸ êµ¬ì¡°]

<img src="./pics/Screen Shot 2021-10-09 at 11.31.20 PM.png" alt="Screen Shot 2021-10-09 at 11.31.20 PM" style="zoom:35%;" />

- Weight & Bias ì´ˆê¸°ê°’ ì„¤ì • :

  ```python
  if isinstance(m, nn.Linear):
  	nn.init.kaiming_uniform_(m.weight.data)
  	nn.init.constant_(m.bias.data, 0)
  ```

- Hidden Layer Node ê°œìˆ˜ ì„¤ì • :

  ```python
  input_feature = 2211
  output_classes = 70
  hidden_nodes = int((input_feature + output_classes) * (2/3))
  ```

- Loss Function ì„¤ì • : `torch.nn.BCELoss()`

- Optimizer ì„¤ì • : `torch.optim.SGD()`

  - í›ˆë ¨ ì¤‘ learning rate ì¡°ì •

    ```python
    lr_init = 0.1
    lr_adjust_rate = 0.01
    lr_adjust_freq = 5
    def __adjust_lr__(self, curr_epoch):
    	lr_curr = lr_init * (lr_adjust_rate ** int(curr_epoch / lr_adjust_freq))
    ```

    
#### CNNì„ ì‚¬ìš©í•œ êµ¬ì¡° ( `model_ver: 2` )

- `model_ver: 1` ë¥¼ í†µí•´ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ êµ¬ìƒí•œ featureë“¤ì„ ê³„ì‚°í•˜ì—¬ ì¶”ì¶œí•˜ëŠ” ë°©ì‹ì€ ì„±ëŠ¥ì´ ì¢‹ì§€ëª»í•œ ê±¸ë¡œ í™•ì¸í•´ CNNì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ ì‹œì¼œë³´ê¸°ë¡œ í–ˆë‹¤.
- ëª¨ë¸ì˜ ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” Human-Object Proposal ë¶€ë¶„ì´ ì œì™¸ëœ <a href="https://arxiv.org/pdf/1702.05448.pdf">HO-RCNN (Chao et al., 2017)</a> ì˜ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ì˜€ê³  Subjectì™€ Object ì´ë¯¸ì§€ë¥¼ ìœ„í•œ CNNëŠ” ë”°ë¡œ í›ˆë ¨ëœ ResNet-152ë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆë‹¤.

##### [ë°ì´í„° ì „ì²˜ë¦¬]

<img src="./pics/Screen Shot 2021-10-10 at 12.12.10 AM.png" alt="Screen Shot 2021-10-10 at 12.12.10 AM" style="zoom:35%;" />

- ë¨¼ì € ì´ë¯¸ì§€ì—ì„œ Subjectì™€ Object bbox ì¢Œí‘œì— ë”°ë¼ Cropë¥¼ í•œ ë‹¤ìŒ ê°ê° 224 Ã— 224 Ã— 3 sizeë¡œ resizeë¥¼ í•˜ì˜€ë‹¤.

-  Interaction pattern (Subjectì™€ Objectì˜ ê´€ê³„) ë¥¼ ì–»ê¸° ìœ„í•´ì„œ ë‹¤ìŒ ê³¼ì •ë“¤ì´ ì§„í–‰ëœë‹¤ :

  1. ì´ë¯¸ì§€ì—ì„œ Subject bbox ì¢Œí‘œ ë‚´ë¶€ì˜ pixelë“¤ì€ 1ë¡œ ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ Interaction patternì˜ ì²« ë²ˆì§¸ channelì´ ëœë‹¤. 

  2. ì´ë¯¸ì§€ì—ì„œ Object bbox ì¢Œí‘œ ë‚´ë¶€ì˜ pixelë“¤ì€ 1ë¡œ ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ì„¤ì • Interaction patternì˜ ë‘ ë²ˆì§¸ channelì´ ëœë‹¤. 

  3. Attention windowë¥¼ ì°¾ì€ ë‹¤ìŒ Interaction patternì—ì„œ Attention window ì¢Œí‘œì— ë”°ë¼ Cropë¥¼ í•œë‹¤.

     - í•œ ì „ì²´ ì´ë¯¸ì§€ì—ì„œ Subject bbox ì™€ Object bbox ê´€ê³„ ìœ„ì¹˜ê°€ targetìœ¼ë¡œ ë°”ë¡œ ì¡í ìˆ˜ ìˆê²Œ Attention windowì„ ì‚¬ìš©í•œë‹¤

     - Subject bbox ì™€ Object bboxì˜ ì¢Œí‘œë¥¼ í™œìš©í•˜ì—¬ Attention windowë¥¼ ì°¾ì•„ë‚¸ë‹¤

       ```python
       '''
       parameter format :
           sub_region --> [xmin, ymin, xmax, ymax]
           obj_region --> [xmin, ymin, xmax, ymax]
       '''
       def find_attention_window(sub_region, obj_region):
           attention_window = []
           for idx in range(0,2):
               if sub_region[idx] < obj_region[idx]:
                   attention_window = attention_window + [sub_region[idx]]
               else:
                   attention_window = attention_window + [obj_region[idx]]
           for idx in range(2,4):
               if sub_region[idx] > obj_region[idx]:
                   attention_window = attention_window + [sub_region[idx]]
               else:
                   attention_window = attention_window + [obj_region[idx]]
           return attention_window
       ```

  4. Crop í•œ Attention window ë¶€ë¶„ ì•ˆì— Subjectì™€ Object bboxì˜ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ resizeë¥¼ í•˜ê¸° ìœ„í•´ Attention windowì˜ ê°€ë¡œ, ì„¸ë¡œ ê¸¸ì´ ì¤‘ ì§§ì€ ê²ƒì„ ê¸´ ê²ƒê³¼ ë™ì¼í•˜ê²Œ ê¸¸ì´ê°€ ë˜ë„ë¡ ì–‘ìª½ sideì— 0ìœ¼ë¡œ paddingì„ ì¤˜ì„œ ì •ì‚¬ê°í˜• í˜•íƒœë¡œ ë§Œë“  ë‹¤ìŒ 224 Ã— 224 Ã— 2 sizeë¡œ resizeí•œë‹¤.

- ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ë°ì´í„° Example :

  <img src="./pics/0.jpg" width="600" />

  <img src="./pics/12.jpg" width="600" />
  
  
  
- ëª¨ë“  ë°ì´í„° ì „ì²˜ë¦¬í•˜ì—¬ pickleë¥¼ í†µí•´ ì €ì¥í•´ë‘” binary fileì˜ ìš©ëŸ‰ì´ ì „ì²´ ì•½ 20GBì •ë„ ë˜ì„œ Colabì˜ Standard RAM(13.6GB)ìœ¼ë¡œëŠ” í›ˆë ¨ ì§„í–‰ì´ ë¶ˆê°€ëŠ¥í–ˆë‹¤ğŸ™„

  - ê·¸ë˜ì„œ AWS EC2(g4dn.4xlarge)ì˜ í˜ì„ ë¹Œë ¤ ì§„í–‰í•˜ì˜€ë‹¤.

  

##### [ëª¨ë¸ êµ¬ì¡°]

<img src="./pics/Screen Shot 2021-10-09 at 11.55.23 PM.png" alt="Screen Shot 2021-10-09 at 11.55.23 PM" style="zoom:90%;" />

- `Object.js` íŒŒì¼ ì•ˆì— ìˆëŠ” ì—¬ëŸ¬ Object classë“¤ì„ ë³´ì•˜ì„ ë•Œ ëŒ€ë‹¤ìˆ˜ê°€ ImageNetì˜ classë“¤ê³¼ ê²¹ì³ì„œ ImageNetë¡œ Pre-trainedëœ ResNet-152ë¥¼ ì‚¬ìš©í•˜ì—¬ Subjectì™€ Object ì´ë¯¸ì§€ì˜ featureë¥¼ ì¶”ì¶œí•˜ì˜€ë‹¤. 
  - ê·¸ë¦¬ê³  ResNet-152ì˜ WeightëŠ” freezeì‹œì¼œì„œ ( `requires_grad = False` ) ì¶”ê°€ì ìœ¼ë¡œ fine-tuningì„ ì§„í–‰í•˜ì§€ ì•Šì•˜ë‹¤.
- Interaction Patternì˜ Channel ìˆ˜ê°€ 2ê°œì´ê¸° ë•Œë¬¸ì— ResNet-152ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë³„ë„ì˜ Convolutionê³¼ Pooling Layerë¥¼ ë§Œë“¤ì–´ì„œ featureë¥¼ ì¶”ì¶œí•˜ì˜€ë‹¤. 
  - Interaction Patternì˜ featureê°€ ì´ë¯¸ì§€ ì „ì²´ë¥¼ í•¨ì¶•í•  ìˆ˜ ìˆë„ë¡ Global Average Poolingì„ ì¶”ê°€í–ˆë‹¤ ( `nn.AdaptiveAvgPool2d(1)` ).
- Subjectì™€ Object ì´ë¯¸ì§€ì˜ feature ê·¸ë¦¬ê³  Interaction Patternì˜ featureê¹Œì§€ ëª¨ë‘ concatenateí•˜ì—¬ `model_ver: 1` ì—ì„œ ì‚¬ìš©í•˜ì˜€ë˜ Classification Layerì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì—ˆë‹¤.



### Test Result

#### Metrics

* í‰ê°€ì§€í‘œ :  Recall@k

  * 1ê°œì˜ Test dataëŠ” ã€ŠSubject, Objectã€‹ ìœ¼ë¡œ ë˜ì–´ìˆëŠ” í•œê°œì˜ ìŒê³¼ kê°œì˜ Relation classë¥¼ í¬í•¨í•¨
    * këŠ” ê³ ì •ê°’ì´ ì•„ë‹Œ í•­ìƒ 1ì´ìƒì˜ ìì—°ìˆ˜ì„

  ***Example.*** Relation classê°€ [A,B,C,D,E,F,G,H,I,J] ì´ë ‡ê²Œ 10ê°œ ìˆìŒ

  - ì–´ë–¤ í•œ Test sampleì˜ labelì´ [1,1,1,0,0,0,0,0,0,0]ì¼ ê²½ìš°, ì´ Test sampleì˜ classëŠ” [A,B,C]ï¼Œk=3
  - ì´ Test sampleì— ëŒ€í•´ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ vector ê°’ì´ [0.9,0.8,0.1,0.0,0.0,0.0,0.5,0.2,0.0,0.0] ì¼ ê²½ìš° ê°€ì¥ ë†’ê²Œ ì˜ˆì¸¡í•œ k=3ê°œì˜ Relation classì€ [A,B,G]
    - ì˜ˆì¸¡í•œ k=3ê°œì˜ Relation class ì¤‘ [A,B]ëŠ” ì •í™•í•˜ê²Œ ì˜ˆì¸¡ì„ í•˜ì˜€ìŒ 
  - Recall@k = len([A,B]) / len([A,B,C]) = 2/3 â‰ˆ 0.67

#### CNNì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ êµ¬ì¡° ( `model_ver: 1` ) ì„±ëŠ¥

- *Hyperparameter setting* :

  ```yaml
  model_ver:1
  
  #[training setting]
  train_split: train
  train_lr: 0.1
  train_lr_adjust_rate: 0.001
  train_lr_adjust_freq: 5
  train_epoch_num: 70
  train_save_freq: 5
  train_print_freq: 10
  train_batch_size: 50
  
  #[testing setting]
  test_split: val
  test_epoch: 70
  test_batch_size: 50
  ```

- *Result* : ì´ 70 epochì˜ í›ˆë ¨ì„ ì‹œì¼°ê³  40 epoch í›ˆë ¨ì„ ì‹œì¼°ì„ ë•Œ ëª¨ë¸ì´ 0.4401ë¡œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

  <img src="./pics/Screen Shot 2021-10-05 at 12.27.59 AM.png" width="300" />

  

#### CNNì„ ì‚¬ìš©í•œ êµ¬ì¡° ( `model_ver: 2` ) ì„±ëŠ¥

- *Hyperparameter setting* :

  ```yaml
  model_ver:2
  
  #[training setting]
  train_split:train
  train_lr:0.1
  train_lr_adjust_rate:0.001
  train_lr_adjust_freq:5
  train_epoch_num:50
  train_save_freq:5
  train_print_freq:10
  train_batch_size:64
  
  #[testing setting]
  test_split:val
  test_epoch:20#equaltotrain_epoch_num
  test_batch_size:64
  ```

- *Result* : 20 epoch í›ˆë ¨ì„ ì‹œì¼°ì„ ë•Œ ëª¨ë¸ì˜ ì„±ëŠ¥ = 0.4362

  <img src="./pics/Screen Shot 2021-11-03 at 10.38.26 AM.png" width="700" />
    å‚è€ƒ ï¼š í›ˆë ¨ ì‹œê°„ ë° AWS EC2 ê³„ì‚° ë¹„ìš©ì´ í° ê´€ê³„ë¡œ 50 epochê¹Œì§€ ëŒë ¤ë³´ì§€ëŠ” ëª»í–ˆë‹¤.
